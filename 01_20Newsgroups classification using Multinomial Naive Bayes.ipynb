{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generative Models - Text classification using Multinomial Naive Bayes \n",
    "\n",
    "This workbook has been classified into 2 sections:\n",
    "\n",
    "1. Implementation of Multinomial Naive Bayes \n",
    "2. Model improvement using different techniques\n",
    "    * Split the training data set into training and validation set\n",
    "    * Replacing frequency f with log(1+f)\n",
    "    * Removing stop words & reducing the size of the vocabulary\n",
    "    \n",
    "In the first section, I was able to achieve an accuracy of about 78%.\n",
    "\n",
    "In the second section:\n",
    "   * By splitting the training data into training and validation, an accuracy of 85% was achieved\n",
    "   * Replacing the frequency f of a word ina document by log(1+f) didn't make much impact on the accuracy. An accuracy of 79% was achieved\n",
    "   * By removing the stop words and by reducing the size of the vocabulary, I was able to get an accuary of about 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import all the libraries\n",
    "import pandas as pd;\n",
    "import math;\n",
    "import pandas.tools.util as tools;\n",
    "import numpy as np;\n",
    "from sklearn.cross_validation import train_test_split;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Deepthi/Documents/DSE/DSE210 - Statistics and Probability/Day 2/Assignment/Work/20news-bydate/matlab\n"
     ]
    }
   ],
   "source": [
    "%cd 20news-bydate/matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Import all the datasets\n",
    "\n",
    "test_data = pd.read_table('test.data',sep=' ',header = None, names = ['DocIdx','WordIdx','Frequency'])\n",
    "train_data = pd.read_table('train.data',sep=' ',header = None, names = ['DocIdx','WordIdx','Frequency'])\n",
    "\n",
    "test_label = pd.read_table('test.label',sep=' ',header = None, names = ['label_idx'])\n",
    "train_label = pd.read_table('train.label',sep=' ',header = None, names = ['label_idx'])\n",
    "\n",
    "test_map = pd.read_table('test.map',sep=' ',header = None, names = ['class','label_idx'])\n",
    "train_map = pd.read_table('train.map',sep=' ',header = None, names = ['class','label_idx'])\n",
    "\n",
    "vocabulary = pd.read_table('vocabulary.txt',header = None, names = ['words'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Data preparation\n",
    "\n",
    "train_label = train_label.reset_index()\n",
    "train_label = train_label.rename(columns={'index': 'DocIdx', 'label_idx': 'label_idx'})\n",
    "train_label['DocIdx'] = train_label['DocIdx']+1 # To match the document index in the data\n",
    "\n",
    "test_label = test_label.reset_index()\n",
    "test_label = test_label.rename(columns={'index': 'DocIdx', 'label_idx': 'label_idx'})\n",
    "test_label['DocIdx'] = test_label['DocIdx']+1 # To match the document index in the data\n",
    "\n",
    "vocabulary = vocabulary.reset_index()\n",
    "vocabulary = vocabulary.rename(columns = {'index':'WordIdx', 'words':'words'})\n",
    "vocabulary['WordIdx'] = vocabulary['WordIdx']+1\n",
    "\n",
    "## Merge train_data and train_label to get labels for each document in the train_data\n",
    "train_data_2 = pd.merge(train_data, train_label,how = 'left', on='DocIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculate pi (document frequency) - priors\n",
    "pi = pd.DataFrame(train_data_2.groupby(['label_idx']).DocIdx.nunique()).reset_index()\n",
    "pi = pi.rename(columns={'DocIdx':'num_doc'})\n",
    "total_docs = sum(pi.num_doc)\n",
    "pi['pi'] = pi['num_doc']/total_docs \n",
    "pi['log_pi'] = pi['pi'].apply(lambda x: math.log(x)) ## Apply log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Include all the words in the vocabulary to ensure higher accuracy on the test data\n",
    "\n",
    "classes = train_map['label_idx']\n",
    "vocabulary2 = vocabulary['WordIdx']\n",
    "train_data_5 = pd.DataFrame(index= pd.MultiIndex.from_arrays(tools.cartesian_product([vocabulary2.tolist(),classes.tolist()]),names=['WordIdx','label_idx'])).reset_index()\n",
    "train_data_3 = pd.DataFrame(train_data_2.groupby(['label_idx','WordIdx']).Frequency.sum()).reset_index()\n",
    "train_data_4 = pd.merge(train_data_5, train_data_3,how = 'left', on=(['WordIdx','label_idx']))\n",
    "train_data_4['Frequency'].fillna(0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculate conditional probabilities P(word/class)\n",
    "\n",
    "# Numerator = total wi in classj\n",
    "df1 = pd.DataFrame(train_data_4.groupby(['label_idx','WordIdx']).Frequency.sum()).reset_index()\n",
    "\n",
    "# Denominator = total words in classj\n",
    "df2 = pd.DataFrame(train_data_4.groupby(['label_idx']).Frequency.sum()).reset_index()\n",
    "cond_prob = pd.merge(df1,df2,how='left', on = 'label_idx')\n",
    "cond_prob = cond_prob.rename(columns={'label_idx':'label_idx', 'WordIdx': 'WordIdx', 'Frequency_x':'numerator', 'Frequency_y':'denominator'})\n",
    "cond_prob['cond_prob'] = (cond_prob['numerator']+1)/(cond_prob['denominator']+len(vocabulary)) ## Laplace smoothing\n",
    "cond_prob['cond_prob_log'] = cond_prob['cond_prob'].apply(lambda x: math.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create a routine \n",
    "\n",
    "def naivebayes(dat,doc):\n",
    "    mask = dat[dat['DocIdx']==doc]['WordIdx']\n",
    "    cond_prob_doc1 = cond_prob[cond_prob['WordIdx'].isin(mask)]\n",
    "    cond_prob_doc1 = pd.merge(cond_prob_doc1,dat[dat['DocIdx']==doc], how = 'left', on = 'WordIdx')\n",
    "    cond_prob_doc1['calc'] = cond_prob_doc1['Frequency']*cond_prob_doc1['cond_prob_log']\n",
    "    cond_prob_doc1_2 = pd.DataFrame(cond_prob_doc1.groupby(['label_idx']).calc.sum()).reset_index()\n",
    "    cond_prob_doc1_2 = pd.merge(cond_prob_doc1_2,pi,how='left',on='label_idx')\n",
    "    cond_prob_doc1_2['final'] = cond_prob_doc1_2['log_pi']+cond_prob_doc1_2['calc']\n",
    "    return int(cond_prob_doc1_2[cond_prob_doc1_2['final'] == cond_prob_doc1_2['final'].max()]['label_idx'])\n",
    "\n",
    "label = []\n",
    "\n",
    "for i in range(1,test_data['DocIdx'].nunique()+1):\n",
    "    label.append(naivebayes(test_data,i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy % = 78.107928048\n"
     ]
    }
   ],
   "source": [
    "original_label = list(test_label['label_idx'])\n",
    "error_calc = pd.DataFrame({'label':label,'original_label':original_label})\n",
    "error_calc['error'] = (error_calc['label']<>error_calc['original_label']).astype('int')\n",
    "accuracy = 100 - float(error_calc['error'].sum())*100/float(error_calc['label'].count())\n",
    "print 'Accuracy % =', accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model improvement using different techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training data set into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy % = 86.3354037267\n"
     ]
    }
   ],
   "source": [
    "# %load 'Improve performance - Split the train data into test and train data.py'\n",
    "\n",
    "## Import data\n",
    "\n",
    "train_data_raw = pd.read_table('train.data',sep=' ',header = None, names = ['DocIdx','WordIdx','Frequency'])\n",
    "train_label = pd.read_table('train.label',sep=' ',header = None, names = ['label_idx'])\n",
    "train_map = pd.read_table('train.map',sep=' ',header = None, names = ['class','label_idx'])\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "train_label = train_label.reset_index()\n",
    "train_label = train_label.rename(columns={'index': 'DocIdx', 'label_idx': 'label_idx'})\n",
    "train_label['DocIdx'] = train_label['DocIdx']+1 # To match the document index in the data\n",
    "\n",
    "train_docs_to_split = pd.DataFrame(train_data_raw['DocIdx'].unique())\n",
    "train_docs_to_split.columns = ['DocIdx']\n",
    "\n",
    "## To see how the smaller training and validation set impacts the performance\n",
    "## Split the training data into training and validation dataset in the ratio 80:20\n",
    "train_docs, test_docs = train_test_split(train_docs_to_split, test_size = 0.2) \n",
    "\n",
    "train_docs= list(train_docs.reset_index()['DocIdx'])\n",
    "test_docs= list(test_docs.reset_index()['DocIdx'])\n",
    "\n",
    "test_data = train_data_raw[train_data_raw['DocIdx'].isin(test_docs)]\n",
    "train_data = train_data_raw[train_data_raw['DocIdx'].isin(train_docs)]\n",
    "\n",
    "test_label = train_label[train_label['DocIdx'].isin(test_docs)]\n",
    "\n",
    "## Use train data to build a Multinomial Naive Bayes model\n",
    "\n",
    "## Merge train_data and train_label to get labels for each document in the train_data\n",
    "train_data_2 = pd.merge(train_data, train_label,how = 'left', on='DocIdx')\n",
    "\n",
    "vocabulary = pd.DataFrame(train_data['WordIdx'].unique())\n",
    "vocabulary.columns = ['WordIdx']\n",
    "\n",
    "## Calculate pi (document frequency) - priors\n",
    "pi = pd.DataFrame(train_data_2.groupby(['label_idx']).DocIdx.nunique()).reset_index()\n",
    "pi = pi.rename(columns={'DocIdx':'num_doc'})\n",
    "total_docs = sum(pi.num_doc)\n",
    "pi['pi'] = pi['num_doc']/total_docs \n",
    "pi['log_pi'] = pi['pi'].apply(lambda x: math.log(x))\n",
    "\n",
    "## Include all the words in the vocabulary\n",
    "\n",
    "classes = train_map['label_idx']\n",
    "vocabulary2 = vocabulary['WordIdx']\n",
    "train_data_5 = pd.DataFrame(index= pd.MultiIndex.from_arrays(tools.cartesian_product([vocabulary2.tolist(),classes.tolist()]),names=['WordIdx','label_idx'])).reset_index()\n",
    "train_data_3 = pd.DataFrame(train_data_2.groupby(['label_idx','WordIdx']).Frequency.sum()).reset_index()\n",
    "train_data_4 = pd.merge(train_data_5, train_data_3,how = 'left', on=(['WordIdx','label_idx']))\n",
    "train_data_4['Frequency'].fillna(0,inplace=True)\n",
    "\n",
    "## Calculate conditional probabilities P(word/class)\n",
    "\n",
    "# Numerator = total w1 in class1\n",
    "df1 = pd.DataFrame(train_data_4.groupby(['label_idx','WordIdx']).Frequency.sum()).reset_index()\n",
    "\n",
    "# Denominator = total words in class1\n",
    "df2 = pd.DataFrame(train_data_4.groupby(['label_idx']).Frequency.sum()).reset_index()\n",
    "cond_prob = pd.merge(df1,df2,how='left', on = 'label_idx')\n",
    "cond_prob = cond_prob.rename(columns={'label_idx':'label_idx', 'WordIdx': 'WordIdx', 'Frequency_x':'numerator', 'Frequency_y':'denominator'})\n",
    "cond_prob['cond_prob'] = (cond_prob['numerator']+1)/(cond_prob['denominator']+len(vocabulary)) ## Laplace smoothing\n",
    "cond_prob['cond_prob_log'] = cond_prob['cond_prob'].apply(lambda x: math.log(x))\n",
    "\n",
    "## Create a routine \n",
    "\n",
    "def naivebayes(dat,doc):\n",
    "    mask = dat[dat['DocIdx']==doc]['WordIdx']\n",
    "    cond_prob_doc1 = cond_prob[cond_prob['WordIdx'].isin(mask)]\n",
    "    cond_prob_doc1 = pd.merge(cond_prob_doc1,dat[dat['DocIdx']==doc], how = 'left', on = 'WordIdx')\n",
    "    cond_prob_doc1['calc'] = cond_prob_doc1['Frequency']*cond_prob_doc1['cond_prob_log']\n",
    "    cond_prob_doc1_2 = pd.DataFrame(cond_prob_doc1.groupby(['label_idx']).calc.sum()).reset_index()\n",
    "    cond_prob_doc1_2 = pd.merge(cond_prob_doc1_2,pi,how='left',on='label_idx')\n",
    "    cond_prob_doc1_2['final'] = cond_prob_doc1_2['log_pi']+cond_prob_doc1_2['calc']\n",
    "    return int(cond_prob_doc1_2[cond_prob_doc1_2['final'] == cond_prob_doc1_2['final'].max()]['label_idx'])\n",
    "\n",
    "label = []\n",
    "\n",
    "for i in test_data['DocIdx'].unique():\n",
    "    label.append(naivebayes(test_data,i))\n",
    "\n",
    "original_label = list(test_label['label_idx'])\n",
    "error_calc = pd.DataFrame({'label':label,'original_label':original_label})\n",
    "error_calc['error'] = (error_calc['label']<>error_calc['original_label']).astype('int')\n",
    "accuracy = 100 - float(error_calc['error'].sum())*100/float(error_calc['label'].count())\n",
    "print 'Accuracy % =', accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing frequency f with log(1+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy % = 78.7341772152\n"
     ]
    }
   ],
   "source": [
    "# %load 'Improve performance - replace frequency f with (1+f).py'\n",
    "\n",
    "## Improve performance by replacing the frequency f of a word in a document by log(1+f)\n",
    "\n",
    "## Import data\n",
    "\n",
    "test_data = pd.read_table('test.data',sep=' ',header = None, names = ['DocIdx','WordIdx','Frequency'])\n",
    "train_data = pd.read_table('train.data',sep=' ',header = None, names = ['DocIdx','WordIdx','Frequency'])\n",
    "\n",
    "test_label = pd.read_table('test.label',sep=' ',header = None, names = ['label_idx'])\n",
    "train_label = pd.read_table('train.label',sep=' ',header = None, names = ['label_idx'])\n",
    "\n",
    "test_map = pd.read_table('test.map',sep=' ',header = None, names = ['class','label_idx'])\n",
    "train_map = pd.read_table('train.map',sep=' ',header = None, names = ['class','label_idx'])\n",
    "\n",
    "vocabulary = pd.read_table('vocabulary.txt',header = None, names = ['words'])\n",
    "\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "train_label = train_label.reset_index()\n",
    "train_label = train_label.rename(columns={'index': 'DocIdx', 'label_idx': 'label_idx'})\n",
    "train_label['DocIdx'] = train_label['DocIdx']+1 # To match the document index in the data\n",
    "\n",
    "test_label = test_label.reset_index()\n",
    "test_label = test_label.rename(columns={'index': 'DocIdx', 'label_idx': 'label_idx'})\n",
    "test_label['DocIdx'] = test_label['DocIdx']+1 # To match the document index in the data\n",
    "\n",
    "vocabulary = vocabulary.reset_index()\n",
    "vocabulary = vocabulary.rename(columns = {'index':'WordIdx', 'words':'words'})\n",
    "vocabulary['WordIdx'] = vocabulary['WordIdx']+1\n",
    "\n",
    "\n",
    "## Use train data to build a Multinomial Naive Bayes model\n",
    "\n",
    "## Merge train_data and train_label to get labels for each document in the train_data\n",
    "train_data_2 = pd.merge(train_data, train_label,how = 'left', on='DocIdx')\n",
    "\n",
    "\n",
    "train_data['one_plus_freq_log']= train_data['Frequency'].apply(lambda x: math.log(x+1))\n",
    "test_data['one_plus_freq_log']= test_data['Frequency'].apply(lambda x: math.log(x+1))\n",
    "\n",
    "## Calculate pi (document frequency) - priors\n",
    "pi = pd.DataFrame(train_data_2.groupby(['label_idx']).DocIdx.nunique()).reset_index()\n",
    "pi = pi.rename(columns={'DocIdx':'num_doc'})\n",
    "total_docs = sum(pi.num_doc)\n",
    "pi['pi'] = pi['num_doc']/total_docs \n",
    "pi['log_pi'] = pi['pi'].apply(lambda x: math.log(x))\n",
    "\n",
    "\n",
    "## Include all the words in the vocabulary\n",
    "\n",
    "classes = train_map['label_idx']\n",
    "vocabulary2 = vocabulary['WordIdx']\n",
    "train_data_5 = pd.DataFrame(index= pd.MultiIndex.from_arrays(tools.cartesian_product([vocabulary2.tolist(),classes.tolist()]),names=['WordIdx','label_idx'])).reset_index()\n",
    "train_data_3 = pd.DataFrame(train_data_2.groupby(['label_idx','WordIdx']).Frequency.sum()).reset_index()\n",
    "train_data_4 = pd.merge(train_data_5, train_data_3,how = 'left', on=(['WordIdx','label_idx']))\n",
    "train_data_4['Frequency'].fillna(0,inplace=True)\n",
    "\n",
    "## Calculate conditional probabilities P(word/class)\n",
    "\n",
    "# Numerator = total w1 in class1\n",
    "df1 = pd.DataFrame(train_data_4.groupby(['label_idx','WordIdx']).Frequency.sum()).reset_index()\n",
    "\n",
    "# Denominator = total words in class1\n",
    "df2 = pd.DataFrame(train_data_4.groupby(['label_idx']).Frequency.sum()).reset_index()\n",
    "cond_prob = pd.merge(df1,df2,how='left', on = 'label_idx')\n",
    "cond_prob = cond_prob.rename(columns={'label_idx':'label_idx', 'WordIdx': 'WordIdx', 'Frequency_x':'numerator', 'Frequency_y':'denominator'})\n",
    "cond_prob['cond_prob'] = (cond_prob['numerator']+1)/(cond_prob['denominator']+len(vocabulary)) ## Laplace smoothing\n",
    "cond_prob['cond_prob_log'] = cond_prob['cond_prob'].apply(lambda x: math.log(x))\n",
    "\n",
    "## Create a routine \n",
    "\n",
    "def naivebayes(dat,doc):\n",
    "    mask = dat[dat['DocIdx']==doc]['WordIdx']\n",
    "    cond_prob_doc1 = cond_prob[cond_prob['WordIdx'].isin(mask)]\n",
    "    cond_prob_doc1 = pd.merge(cond_prob_doc1,dat[dat['DocIdx']==doc], how = 'left', on = 'WordIdx')\n",
    "    cond_prob_doc1['calc'] = cond_prob_doc1['one_plus_freq_log']+cond_prob_doc1['cond_prob_log']\n",
    "    cond_prob_doc1_2 = pd.DataFrame(cond_prob_doc1.groupby(['label_idx']).calc.sum()).reset_index()\n",
    "    cond_prob_doc1_2 = pd.merge(cond_prob_doc1_2,pi,how='left',on='label_idx')\n",
    "    cond_prob_doc1_2['final'] = cond_prob_doc1_2['log_pi']+cond_prob_doc1_2['calc']\n",
    "    return int(cond_prob_doc1_2[cond_prob_doc1_2['final'] == cond_prob_doc1_2['final'].max()]['label_idx'])\n",
    "\n",
    "label = []\n",
    "\n",
    "for i in range(1,test_data['DocIdx'].nunique()+1):\n",
    "    label.append(naivebayes(test_data,i))\n",
    "\n",
    "\n",
    "original_label = list(test_label['label_idx'])\n",
    "error_calc = pd.DataFrame({'label':label,'original_label':original_label})\n",
    "error_calc['error'] = (error_calc['label']<>error_calc['original_label']).astype('int')\n",
    "accuracy = 100 - float(error_calc['error'].sum())*100/float(error_calc['label'].count())\n",
    "print 'Accuracy % =', accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words & reducing the size of the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy % = 80.2905117271\n"
     ]
    }
   ],
   "source": [
    "# %load 'Improve performance - Remove stopwords.py'\n",
    "## Improve performance by removing stop words\n",
    "\n",
    "## Import data\n",
    "test_data_raw = pd.read_table('test.data',sep=' ',header = None, names = ['DocIdx','WordIdx','Frequency'])\n",
    "train_data_raw = pd.read_table('train.data',sep=' ',header = None, names = ['DocIdx','WordIdx','Frequency'])\n",
    "\n",
    "test_label = pd.read_table('test.label',sep=' ',header = None, names = ['label_idx'])\n",
    "train_label = pd.read_table('train.label',sep=' ',header = None, names = ['label_idx'])\n",
    "\n",
    "test_map = pd.read_table('test.map',sep=' ',header = None, names = ['class','label_idx'])\n",
    "train_map = pd.read_table('train.map',sep=' ',header = None, names = ['class','label_idx'])\n",
    "\n",
    "vocabulary = pd.read_table('vocabulary.txt',header = None, names = ['words'])\n",
    "\n",
    "stop_words = pd.read_table(\"stop_words.txt\", header = None, names = ['stop_words'])\n",
    "stop_words = list(stop_words['stop_words'])\n",
    "\n",
    "vocabulary2 = vocabulary[~vocabulary['words'].isin(stop_words)]\n",
    "\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "train_label = train_label.reset_index()\n",
    "train_label = train_label.rename(columns={'index': 'DocIdx', 'label_idx': 'label_idx'})\n",
    "train_label['DocIdx'] = train_label['DocIdx']+1 # To match the document index in the data\n",
    "\n",
    "test_label = test_label.reset_index()\n",
    "test_label = test_label.rename(columns={'index': 'DocIdx', 'label_idx': 'label_idx'})\n",
    "test_label['DocIdx'] = test_label['DocIdx']+1 # To match the document index in the data\n",
    "\n",
    "vocabulary2 = vocabulary2.reset_index()\n",
    "vocabulary2 = vocabulary2.rename(columns = {'index':'WordIdx', 'words':'words'})\n",
    "vocabulary2['WordIdx'] = vocabulary2['WordIdx']+1\n",
    "\n",
    "\n",
    "test_data = test_data_raw[test_data_raw['WordIdx'].isin(list(vocabulary2['WordIdx']))]\n",
    "train_data = train_data_raw[train_data_raw['WordIdx'].isin(list(vocabulary2['WordIdx']))]\n",
    "\n",
    "## Use train data to build a Multinomial Naive Bayes model\n",
    "\n",
    "## Merge train_data and train_label to get labels for each document in the train_data\n",
    "train_data_2 = pd.merge(train_data, train_label,how = 'left', on='DocIdx')\n",
    "\n",
    "test_label = test_label[test_label['DocIdx'].isin(list(test_data['DocIdx']))]\n",
    "\n",
    "train_label = train_label[train_label['DocIdx'].isin(list(train_data['DocIdx']))]\n",
    "\n",
    "## Calculate pi (document frequency) - priors\n",
    "pi = pd.DataFrame(train_data_2.groupby(['label_idx']).DocIdx.nunique()).reset_index()\n",
    "pi = pi.rename(columns={'DocIdx':'num_doc'})\n",
    "total_docs = sum(pi.num_doc)\n",
    "pi['pi'] = pi['num_doc']/total_docs \n",
    "pi['log_pi'] = pi['pi'].apply(lambda x: math.log(x))\n",
    "\n",
    "\n",
    "## Include all the words in the vocabulary\n",
    "\n",
    "classes = train_map['label_idx']\n",
    "vocabulary2 = vocabulary2['WordIdx']\n",
    "train_data_5 = pd.DataFrame(index= pd.MultiIndex.from_arrays(tools.cartesian_product([vocabulary2.tolist(),classes.tolist()]),names=['WordIdx','label_idx'])).reset_index()\n",
    "train_data_3 = pd.DataFrame(train_data_2.groupby(['label_idx','WordIdx']).Frequency.sum()).reset_index()\n",
    "train_data_4 = pd.merge(train_data_5, train_data_3,how = 'left', on=(['WordIdx','label_idx']))\n",
    "train_data_4['Frequency'].fillna(0,inplace=True)\n",
    "\n",
    "## Calculate conditional probabilities P(word/class)\n",
    "\n",
    "# Numerator = total w1 in class1\n",
    "df1 = pd.DataFrame(train_data_4.groupby(['label_idx','WordIdx']).Frequency.sum()).reset_index()\n",
    "\n",
    "# Denominator = total words in class1\n",
    "df2 = pd.DataFrame(train_data_4.groupby(['label_idx']).Frequency.sum()).reset_index()\n",
    "cond_prob = pd.merge(df1,df2,how='left', on = 'label_idx')\n",
    "cond_prob = cond_prob.rename(columns={'label_idx':'label_idx', 'WordIdx': 'WordIdx', 'Frequency_x':'numerator', 'Frequency_y':'denominator'})\n",
    "cond_prob['cond_prob'] = (cond_prob['numerator']+1)/(cond_prob['denominator']+len(vocabulary)) ## Laplace smoothing\n",
    "cond_prob['cond_prob_log'] = cond_prob['cond_prob'].apply(lambda x: math.log(x))\n",
    "\n",
    "## Create a routine \n",
    "\n",
    "def naivebayes(dat,doc):\n",
    "    mask = dat[dat['DocIdx']==doc]['WordIdx']\n",
    "    cond_prob_doc1 = cond_prob[cond_prob['WordIdx'].isin(mask)]\n",
    "    cond_prob_doc1 = pd.merge(cond_prob_doc1,dat[dat['DocIdx']==doc], how = 'left', on = 'WordIdx')\n",
    "    cond_prob_doc1['calc'] = cond_prob_doc1['Frequency']*cond_prob_doc1['cond_prob_log']\n",
    "    cond_prob_doc1_2 = pd.DataFrame(cond_prob_doc1.groupby(['label_idx']).calc.sum()).reset_index()\n",
    "    cond_prob_doc1_2 = pd.merge(cond_prob_doc1_2,pi,how='left',on='label_idx')\n",
    "    cond_prob_doc1_2['final'] = cond_prob_doc1_2['log_pi']+cond_prob_doc1_2['calc']\n",
    "    return int(cond_prob_doc1_2[cond_prob_doc1_2['final'] == cond_prob_doc1_2['final'].max()]['label_idx'])\n",
    "\n",
    "label = []\n",
    "\n",
    "for i in test_data['DocIdx'].unique():\n",
    "    label.append(naivebayes(test_data,i))\n",
    "\n",
    "original_label = list(test_label['label_idx'])\n",
    "error_calc = pd.DataFrame({'label':label,'original_label':original_label})\n",
    "error_calc['error'] = (error_calc['label']<>error_calc['original_label']).astype('int')\n",
    "accuracy = 100 - float(error_calc['error'].sum())*100/float(error_calc['label'].count())\n",
    "print 'Accuracy % =', accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
